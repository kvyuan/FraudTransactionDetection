{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries for plotting\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries for modeling\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, SparkSession, Window, Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import pyspark.sql.functions as F\n",
    "import itertools\n",
    "from itertools import repeat\n",
    "import pickle\n",
    "import pyspark\n",
    "\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "class CreateBestModel:\n",
    "    def __init__(self, algo, avgprecision, avgrecall, avgfscore, hyperparams, ootmodel, ootprecision, ootrecall, ootfscore):\n",
    "        self.algo = algo\n",
    "        self.gsPrecision = avgprecision\n",
    "        self.gsFScore = avgfscore\n",
    "        self.gsRecall = avgrecall\n",
    "        self.hyperParams = hyperparams\n",
    "        self.model = ootmodel\n",
    "        self.ootPrecision = ootprecision\n",
    "        self.ootFScore = ootfscore\n",
    "        self.ootRecall = ootrecall\n",
    "\n",
    "#function-based\n",
    "def sample(df, sampling_method, ratio):\n",
    "\n",
    "    notfraud = df.select('*').where(df.Class == 0.0)\n",
    "    fraud = df.select('*').where(df.Class == 1.0)\n",
    "\n",
    "    if sampling_method == \"over\":\n",
    "        nrows = notfraud.select(\"Class\").count()\n",
    "        sample_size = int(nrows*ratio/(1-ratio))\n",
    "        sampled = fraud.rdd.takeSample(True, sample_size, 46)\n",
    "        fraud = sqlContext.createDataFrame(sampled)\n",
    "\n",
    "    elif sampling_method == \"under\":\n",
    "        nrows = fraud.select(\"Class\").count()\n",
    "        sample_size = int(nrows*(1-ratio)/ratio)\n",
    "        sampled = notfraud.rdd.takeSample(False, sample_size, 46)\n",
    "        notfraud = sqlContext.createDataFrame(sampled)\n",
    "    else:\n",
    "        return df\n",
    "    \n",
    "    sampled = fraud.union(notfraud)\n",
    "\n",
    "    #shuffle undersampled dataframe\n",
    "    nrows = sampled.select(\"Class\").count()\n",
    "    shuffled = sampled.rdd.takeSample(False, nrows, 46)\n",
    "    shuffled_df = sqlContext.createDataFrame(shuffled)\n",
    "\n",
    "    return shuffled_df\n",
    "\n",
    "def generateParamGrid(*args):\n",
    "    \n",
    "    grid = list(itertools.product(*args))\n",
    "    return grid\n",
    "\n",
    "def generateClassifier(algo, params, features):\n",
    "\n",
    "    ############################################################################\n",
    "    #TODO: complete this section\n",
    "\n",
    "    def lr(params,features):\n",
    "        print(params)\n",
    "        if len(params) > 2:\n",
    "            lrClassifier = LogisticRegression(featuresCol = 'features',\n",
    "                                          labelCol = 'Class',\n",
    "                                          threshold=params[0],\n",
    "                                           maxIter=params[1],\n",
    "                                           weightCol=params[2])\n",
    "                                          #regParam=params[2])\n",
    "                                          #elasticNetParam=params[2])\n",
    "        else:\n",
    "            lrClassifier = LogisticRegression(featuresCol = 'features',\n",
    "                                          labelCol = 'Class',\n",
    "                                          threshold=params[0],\n",
    "                                           maxIter=params[1])\n",
    "        return lrClassifier\n",
    "\n",
    "\n",
    "    def gbm(params,features):\n",
    "        gbmClassifier = GBTClassifier(featuresCol = 'features',\n",
    "                                      labelCol = 'Class',\n",
    "                                      maxDepth = params[0],\n",
    "                                      minInfoGain = params[1])\n",
    "        return gbmClassifier\n",
    "\n",
    "    def rf(params,features):\n",
    "        rfClassifier = RandomForestClassifier(featuresCol='features',\n",
    "                                              labelCol='Class',\n",
    "                                              maxDepth=params[0],\n",
    "                                              minInfoGain=params[1],\n",
    "                                              numTrees=params[2])\n",
    "\n",
    "        return rfClassifier\n",
    "\n",
    "    def mlp(params,features):\n",
    "        input_layers = len(features)\n",
    "        layers = [input_layers, params[1], 2]\n",
    "        print(layers)\n",
    "        mlpClassifier = MultilayerPerceptronClassifier(featuresCol = 'features',\n",
    "                                                       labelCol = 'Class',\n",
    "                                                       maxIter = params[0],\n",
    "                                                       layers = layers,\n",
    "                                                       stepSize = params[2])\n",
    "        return mlpClassifier\n",
    "\n",
    "    def svm(params, features):\n",
    "        svmClassifier = LinearSVC(featuresCol = 'features',\n",
    "                         labelCol='Class', \n",
    "                         maxIter=params[0],\n",
    "                         regParam=params[1],\n",
    "                         tol =params[2]\n",
    "                         )\n",
    "        \n",
    "        return svmClassifier\n",
    "\n",
    "    def xg(params,features):\n",
    "        return\n",
    "    ############################################################################\n",
    "\n",
    "    getClassifier = {\n",
    "        'lr':lr,\n",
    "        'gbm':gbm,\n",
    "        'rf':rf,\n",
    "        'mlp':mlp,\n",
    "        'svm':svm,\n",
    "        'xg':xg}\n",
    "\n",
    "    return getClassifier[algo](params,features)\n",
    "\n",
    "def crossValidate(df, folds, k, classifier, features, sampling_method, ratio, pool):\n",
    "\n",
    "    def build(fold, df, classifier, features, sampling_method, ratio):\n",
    "\n",
    "        validation = fold\n",
    "        train = df.subtract(fold)\n",
    "\n",
    "#         #add class weight\n",
    "#         notfraud_count = train.select(\"Class\").where(train.Class == 0.0).count()\n",
    "#         total_count = train.select(\"Class\").count()\n",
    "#         balance_ratio = notfraud_count / total_count\n",
    "#         train=train.withColumn(\"classWeights\", F.when(train.Class == 1.0,balance_ratio).otherwise(1-balance_ratio))\n",
    "        \n",
    "        train = sample(train, sampling_method, ratio)\n",
    "        fraud_count = train.select(\"Class\").where(train.Class == 1.0).count()\n",
    "        tot_count = train.select(\"Class\").count()\n",
    "        fraud_ratio = fraud_count / tot_count\n",
    "        print(\"train: \" + str(tot_count))\n",
    "        print(\"fraud ratio: \" + str(fraud_ratio))\n",
    "        \n",
    "        vectorAssembler = VectorAssembler(inputCols = features, outputCol = 'features')\n",
    "        vector_train = vectorAssembler.transform(train)\n",
    "        vector_validate = vectorAssembler.transform(validation)\n",
    "        model = classifier.fit(vector_train)\n",
    "        pred = model.transform(vector_validate)\n",
    "        pos = pred.filter(pred.prediction == 1.0).count()\n",
    "        if pos != 0:\n",
    "            precision = pred.filter(pred.Class == pred.prediction).filter(pred.Class == 1.0).count() / pos\n",
    "        else:\n",
    "            precision = 0\n",
    "        fraud = pred.filter(pred.Class == 1.0).count()\n",
    "        if fraud != 0:\n",
    "            recall = pred.filter(pred.Class == pred.prediction).filter(pred.Class == 1.0).count() / fraud\n",
    "        else:\n",
    "            recall = 0\n",
    "        precision_recall = precision + recall\n",
    "        if precision_recall != 0:\n",
    "            f_score = 2 * precision * recall /(precision_recall)\n",
    "        else:\n",
    "            f_score = 0\n",
    "        #print(\"\\n precision, recall, f_score: \" + str(precision) + \", \" + str(recall) + \", \" + str(f_score))\n",
    "        return [precision, recall, f_score]\n",
    "\n",
    "    #call multiprocessing here\n",
    "    cvperformance = pool.map(lambda fold: build(fold, df, classifier, features, sampling_method, ratio), folds)\n",
    "\n",
    "    #calculate metrics\n",
    "    precision_sum = sum([x[0] for x in cvperformance])\n",
    "    recall_sum = sum([x[1] for x in cvperformance])\n",
    "\n",
    "    avg_precision = precision_sum/k\n",
    "    avg_recall = recall_sum/k\n",
    "    if avg_precision+avg_recall == 0:\n",
    "        avg_fscore = 0\n",
    "    else:\n",
    "        avg_fscore = 2 * avg_precision * avg_recall /(avg_precision+avg_recall)\n",
    "    return [avg_precision,avg_recall,avg_fscore]\n",
    "\n",
    "def gridSearch(df, folds, k, algo, grid, features, sampling_method, ratio, pool):\n",
    "\n",
    "    best_hyper = None\n",
    "    best_precision = 0\n",
    "    best_recall = 0\n",
    "    best_fscore = 0\n",
    "\n",
    "    for i in range(len(grid)):\n",
    "        params = list(grid[i])\n",
    "        print(params)\n",
    "        classifier = generateClassifier(algo, params, features)\n",
    "        modelPerformance = crossValidate(df, folds, k, classifier, features, sampling_method, ratio, pool)\n",
    "        print(modelPerformance)\n",
    "        if modelPerformance[2] > best_fscore:\n",
    "            best_hyper = params\n",
    "            best_precision = modelPerformance[0]\n",
    "            best_recall = modelPerformance[1]\n",
    "            best_fscore = modelPerformance[2]\n",
    "\n",
    "    return best_hyper, best_precision, best_recall, best_fscore\n",
    "\n",
    "def TrainTest(traindf,testdf,algo,features,params):\n",
    "    vectorAssembler = VectorAssembler(inputCols = features, outputCol = 'features')\n",
    "    classifier = generateClassifier(algo, params, features)\n",
    "    vector_train = vectorAssembler.transform(traindf)\n",
    "    vector_test = vectorAssembler.transform(testdf)\n",
    "    m = classifier.fit(vector_train)\n",
    "    pred = m.transform(vector_test)\n",
    "    pos = pred.filter(pred.prediction == 1.0).count()\n",
    "    if pos != 0:\n",
    "        precision = pred.filter(pred.Class == pred.prediction).filter(pred.Class == 1.0).count() / pos\n",
    "    else:\n",
    "        precision = 0\n",
    "    fraud = pred.filter(pred.Class == 1.0).count()\n",
    "    if fraud != 0:\n",
    "        recall = pred.filter(pred.Class == pred.prediction).filter(pred.Class == 1.0).count() / fraud\n",
    "    else:\n",
    "        recall = 0\n",
    "    precision_recall = precision + recall\n",
    "    if precision_recall != 0:\n",
    "        f_score = 2 * precision * recall /(precision_recall)\n",
    "    else:\n",
    "        f_score = 0\n",
    "    print(\"\\n precision, recall, f_score: \" + str(precision) + \", \" + str(recall) + \", \" + str(f_score))\n",
    "    predictionAndLabels = pred.select('Class','prediction').rdd.map(lambda lp: (float(lp.prediction), lp.Class))\n",
    "    metrics = MulticlassMetrics(predictionAndLabels)\n",
    "    print(metrics.confusionMatrix().toArray())\n",
    "    #evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"Class\",metricName=\"areaUnderPR\")\n",
    "    #print(\"Area under Precision Recall Curve = %s\" % evaluator.evaluate(pred))\n",
    "    \n",
    "    return m, precision, recall, f_score\n",
    "\n",
    "def tune(df, k, stratification_flag, sampling_method, ratio, modelobj_flag, features, algo, *args, **kwargs):\n",
    "\n",
    "    \"\"\"\n",
    "    Entry point of this suite of functions. returns cv metrics or a model object\n",
    "    Example:\n",
    "        >>> cv_hyper, cv_precision, cv_recall, cv_fscore = tune(df, 5, True,\n",
    "        'None', 0, False, features, 'mlp', [100], [15], [0.03])\n",
    "    :param df: data for modeling purpose\n",
    "    :type df: : pyspark dataframe\n",
    "    :param k: number of folds for cross validation\n",
    "    :type k: int\n",
    "    :param stratification_flag: specifies whether fraud ratio is fixed for each fold. True for stratification\n",
    "    :type stratification_flag: boolean\n",
    "    :param sampling_method: \"over\" for oversampling minority class, \"under\" for undersampling majority class, \"None\"\n",
    "    :type sampling_method: str\n",
    "    :param ratio: targeted fraud ratio after sampling.\n",
    "    :type ratio: float\n",
    "    :param modelobj_flag: specifies whether to return a model object for out of time test. if False, returns cv performancce\n",
    "    :type modelobj_flag: float\n",
    "    :param features: features for training\n",
    "    :type features: list\n",
    "    :param *args: a sequence of params for hyperparams tuning. ex. [values for params1], [values for params2],...\n",
    "    :type *args: list\n",
    "    :returns: model object or cross validation metrics depending on modelobj_flag\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    pool = ThreadPool(3)\n",
    "\n",
    "    #reduce df dimenions to include features and class\n",
    "    cols = features+['Class', 'index']\n",
    "    df = df.select(cols)\n",
    "    df = df.select(*(F.col(c).cast(\"double\").alias(c) for c in df.columns))\n",
    "    df.cache()\n",
    "    #df.drop(\"index\")\n",
    "    \n",
    "    ########################ClassWeights#################################\n",
    "    if algo in [\"lr\", \"svm\"] and [\"ClassWeigts\"] in args:\n",
    "        #add class weight\n",
    "        balance_ratio = args[-1][0]\n",
    "        df=df.withColumn(\"classWeights\", when(df.Class == 1.0,balance_ratio).otherwise(1-balance_ratio))\n",
    "    ########################ClassWeights#################################\n",
    "    \n",
    "    folds = []\n",
    "    \n",
    "    if stratification_flag == False:\n",
    "        tot_count = df.select(\"Class\").count()\n",
    "        n = int(tot_count / k)\n",
    "\n",
    "        #create sub-dataframe iteratively\n",
    "        fold_start = 1\n",
    "        fold_end = n\n",
    "        for i in range(k):\n",
    "            fold = df.select('*').where(df.index.between(fold_start, fold_end))\n",
    "            folds.append(fold)\n",
    "            fold_start = fold_end + 1\n",
    "            fold_end = fold_start + n\n",
    "            if i == k-2:\n",
    "                end = tot_count\n",
    "                \n",
    "    if stratification_flag == True:\n",
    "        fraud = df.select(\"*\").where(df.Class == 1.0)\n",
    "        #shuffle undersampled dataframe\n",
    "        nrows = fraud.select(\"Class\").count()\n",
    "        shuffled = fraud.rdd.takeSample(False, nrows, 46)\n",
    "        fraud = sqlContext.createDataFrame(shuffled)\n",
    "        #add row index to dataframe\n",
    "        fraud = fraud.withColumn('dummy', F.lit('7'))\n",
    "        fraud = fraud.withColumn(\"temp_index\", F.row_number().over(Window.partitionBy(\"dummy\").orderBy(\"dummy\")))\n",
    "        fraud = fraud.drop('dummy')\n",
    "        fraud_count = fraud.select(\"Class\").count()\n",
    "        each_fraud = int(fraud_count/k)\n",
    "\n",
    "        notfraud = df.select(\"*\").where(df.Class == 0.0)\n",
    "        nrows = notfraud.select(\"Class\").count()\n",
    "        shuffled = notfraud.rdd.takeSample(False, nrows, 46)\n",
    "        notfraud = sqlContext.createDataFrame(shuffled)\n",
    "        #add row index to dataframe\n",
    "        notfraud = notfraud.withColumn('dummy', F.lit('7'))\n",
    "        notfraud = notfraud.withColumn(\"temp_index\", F.row_number().over(Window.partitionBy(\"dummy\").orderBy(\"dummy\")))\n",
    "        notfraud = notfraud.drop('dummy')\n",
    "        notfraud_count = notfraud.select(\"Class\").count()\n",
    "        each_notfraud = int(notfraud_count/k)\n",
    "\n",
    "        fraud_start = 1\n",
    "        fraud_end = each_fraud\n",
    "        notfraud_start = 1\n",
    "        notfraud_end = each_notfraud\n",
    "\n",
    "        for i in range(k):\n",
    "            fraud_fold  = fraud.select('*').where(fraud.temp_index.between(fraud_start, fraud_end))\n",
    "            notfraud_fold = notfraud.select('*').where(notfraud.temp_index.between(notfraud_start, notfraud_end))\n",
    "            fold = fraud_fold.union(notfraud_fold).drop(\"temp_index\")\n",
    "            folds.append(fold)\n",
    "            fraud_start = fraud_end + 1\n",
    "            fraud_end = fraud_start + each_fraud\n",
    "            notfraud_start = notfraud_end + 1\n",
    "            notfraud_end = notfraud_start + each_notfraud\n",
    "            if i == k-2:\n",
    "                fraud_end = fraud_count\n",
    "                notfraud_end = notfraud_count\n",
    "\n",
    "\n",
    "    #generate hyperparam combo\n",
    "    grid = generateParamGrid(*args)\n",
    "\n",
    "    #conduct grid search:\n",
    "    best_hyper, best_precision, best_recall, best_fscore = gridSearch(df, folds, k, algo, grid, features, sampling_method, ratio, pool)\n",
    "\n",
    "    if modelobj_flag == True:\n",
    "        #generate a model obj\n",
    "        traindf = sample(df, sampling_method, ratio)\n",
    "        testdf = sqlContext.read.csv(\"oot.csv\", header = True)\n",
    "        cols = features+['Class', 'index']\n",
    "        testdf = testdf.select(cols)\n",
    "        testdf = testdf.select(*(F.col(c).cast(\"double\").alias(c) for c in testdf.columns))\n",
    "        model, precision, recall, fscore = ootTest(traindf, testdf, algo,features,best_hyper)\n",
    "        \n",
    "        modelobj = CreateBestModel(algo, best_precision, best_recall, best_fscore, best_hyper, \n",
    "                                   model, precision, recall, fscore)\n",
    "        return modelobj\n",
    "\n",
    "    return best_hyper, best_precision, best_recall, best_fscore\n",
    "\n",
    "def save(content, filename):\n",
    "\n",
    "    pickle.dump(content, open(filename, \"wb\"))\n",
    "\n",
    "def load(filename):\n",
    "\n",
    "    content = pickle.load(open(filename, \"rb\"))\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15g\n"
     ]
    }
   ],
   "source": [
    "sc=pyspark.SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "print(sc._conf.get('spark.executor.memory'))\n",
    "trainData = sqlContext.read.csv(\"base_train.csv\", header = True)\n",
    "testData = sqlContext.read.csv(\"base_test.csv\", header = True)\n",
    "features = ['Amount', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', \n",
    "            'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28']\n",
    "oot = sqlContext.read.csv(\"oot.csv\", header = True)\n",
    "cols = features+['Class', 'index']\n",
    "trainData = trainData.select(cols)\n",
    "trainData = trainData.select(*(F.col(c).cast(\"double\").alias(c) for c in trainData.columns))\n",
    "testData = testData.select(cols)\n",
    "testData = testData.select(*(F.col(c).cast(\"double\").alias(c) for c in testData.columns))\n",
    "oot = oot.select(cols)\n",
    "oot = oot.select(*(F.col(c).cast(\"double\").alias(c) for c in oot.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Base Model: Interpolation using default params [100, 15, 0.03]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29, 15, 2]\n",
      "\n",
      " precision, recall, f_score: 0.8735632183908046, 0.8351648351648352, 0.853932584269663\n",
      "[[4.5294e+04 1.1000e+01]\n",
      " [1.5000e+01 7.6000e+01]]\n"
     ]
    }
   ],
   "source": [
    "features = ['Amount', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', \n",
    "            'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28']\n",
    "#maxIter, number of neurons in hidden layer, stepsize\n",
    "params = [100, 15, 0.03]\n",
    "m, p, r, f = TrainTest(trainData,testData,'mlp',features,params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random vs stratified 5 fold cross validation using default params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 15, 0.03]\n",
      "[29, 15, 2]\n",
      "train: 152562\n",
      "fraud ratio: 0.0014420366801693738\n",
      "train: 152538\n",
      "fraud ratio: 0.0017372720240202441\n",
      "train: 152482\n",
      "fraud ratio: 0.0016592122348867407\n",
      "train: 152553\n",
      "fraud ratio: 0.0018223174896593315\n",
      "train: 152499\n",
      "fraud ratio: 0.0016655846923586384\n",
      "[0.8855566097406704, 0.7239147286821705, 0.7966186878497847]\n",
      "avg precision: 0.8855566097406704\n",
      "avg recall: 0.7239147286821705\n",
      "avg f-score: 0.7966186878497847\n"
     ]
    }
   ],
   "source": [
    "sqlContext.clearCache()\n",
    "\n",
    "df = sqlContext.read.csv(\"base_train.csv\", header = True)\n",
    "features = ['Amount', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28']\n",
    "hiddenlayer = int((len(features) + 2) / 2)\n",
    "cv_hyper, cv_precision, cv_recall, cv_fscore = tune(df, 5, False, 'None', 0, False, features, 'mlp', [100], [hiddenlayer], [0.03])\n",
    "print(\"avg precision:\", cv_precision)\n",
    "print(\"avg recall:\", cv_recall)\n",
    "print(\"avg f-score:\", cv_fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stratified 5 fold cross validation using default params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 15, 0.03]\n",
      "[29, 15, 2]\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145268\n",
      "fraud ratio: 0.001700305641985847\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145270\n",
      "fraud ratio: 0.0017002822330832244\n",
      "[0.898414964064903, 0.7662612374405076, 0.8270924537479238]\n",
      "avg precision: 0.898414964064903\n",
      "avg recall: 0.7662612374405076\n",
      "avg f-score: 0.8270924537479238\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.csv(\"base_train.csv\", header = True)\n",
    "features = ['Amount', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', \n",
    "            'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', \n",
    "            'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', \n",
    "            'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28']\n",
    "hiddenlayer = int((len(features) + 2) / 2)\n",
    "cv_hyper, cv_precision, cv_recall, cv_fscore = tune(df, 5, True, 'None', 0, False, features, 'mlp', [100], [hiddenlayer], [0.03])\n",
    "print(\"avg precision:\", cv_precision)\n",
    "print(\"avg recall:\", cv_recall)\n",
    "print(\"avg f-score:\", cv_fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 14, 0.03]\n",
      "[29, 14, 2]\n",
      "train: 145268\n",
      "fraud ratio: 0.001700305641985847\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145270\n",
      "fraud ratio: 0.0017002822330832244\n",
      "[0.8660056701164771, 0.763088313061872, 0.8112961102734016]\n",
      "[100, 14, 0.01]\n",
      "[29, 14, 2]\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145268\n",
      "fraud ratio: 0.001700305641985847\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145270\n",
      "fraud ratio: 0.0017002822330832244\n",
      "[0.8669156802277007, 0.7727657324167108, 0.8171376773665636]\n",
      "[100, 15, 0.03]\n",
      "[29, 15, 2]\n",
      "train: 145268\n",
      "fraud ratio: 0.001700305641985847\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145270\n",
      "fraud ratio: 0.0017002822330832244\n",
      "[0.88490513356185, 0.7695399259650978, 0.8232002955263191]\n",
      "[100, 15, 0.01]\n",
      "[29, 15, 2]\n",
      "train: 145268\n",
      "fraud ratio: 0.001700305641985847\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145270\n",
      "fraud ratio: 0.0017002822330832244\n",
      "[0.8786440818355429, 0.749920676890534, 0.8091951653324467]\n",
      "[100, 16, 0.03]\n",
      "[29, 16, 2]\n",
      "train: 145268\n",
      "fraud ratio: 0.001700305641985847\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145270\n",
      "fraud ratio: 0.0017002822330832244\n",
      "[0.8838432815643846, 0.7662083553675304, 0.8208326236738631]\n",
      "[100, 16, 0.01]\n",
      "[29, 16, 2]\n",
      "train: 145268\n",
      "fraud ratio: 0.001700305641985847\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145270\n",
      "fraud ratio: 0.0017002822330832244\n",
      "[0.8871944134103227, 0.7433104177683765, 0.8089038897199335]\n",
      "[200, 14, 0.03]\n",
      "[29, 14, 2]\n",
      "train: 145268\n",
      "fraud ratio: 0.001700305641985847\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145270\n",
      "fraud ratio: 0.0017002822330832244\n",
      "[0.8966719585271669, 0.7630354309888948, 0.8244736134240269]\n",
      "[200, 14, 0.01]\n",
      "[29, 14, 2]\n",
      "train: 145268\n",
      "fraud ratio: 0.001700305641985847\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145270\n",
      "fraud ratio: 0.0017002822330832244\n",
      "[0.8955369075369075, 0.7598625066102592, 0.8221398576168623]\n",
      "[200, 15, 0.03]\n",
      "[29, 15, 2]\n",
      "train: 145268\n",
      "fraud ratio: 0.001700305641985847\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145270\n",
      "fraud ratio: 0.0017002822330832244\n",
      "[0.9032176319339369, 0.740137493389741, 0.8135858449382106]\n",
      "[200, 15, 0.01]\n",
      "[29, 15, 2]\n",
      "train: 145268\n",
      "fraud ratio: 0.001700305641985847\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145270\n",
      "fraud ratio: 0.0017002822330832244\n",
      "[0.9111658330943104, 0.736911686938128, 0.8148266607419016]\n",
      "[200, 16, 0.03]\n",
      "[29, 16, 2]\n",
      "train: 145268\n",
      "fraud ratio: 0.001700305641985847\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145270\n",
      "fraud ratio: 0.0017002822330832244\n",
      "[0.9231637587069723, 0.7305129561078794, 0.8156165957990119]\n",
      "[200, 16, 0.01]\n",
      "[29, 16, 2]\n",
      "train: 145268\n",
      "fraud ratio: 0.001700305641985847\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145270\n",
      "fraud ratio: 0.0017002822330832244\n",
      "[0.9148666971549348, 0.7272342675832892, 0.8103307003998679]\n",
      "gs precision: 0.8966719585271669\n",
      "gs recall: 0.7630354309888948\n",
      "gs f-score: 0.8244736134240269\n",
      "gs hyper: [200, 14, 0.03]\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.csv(\"base_train.csv\", header = True)\n",
    "features = ['Amount', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', \n",
    "            'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', \n",
    "            'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', \n",
    "            'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28']\n",
    "hiddenlayer = int((len(features) + 2) / 2)\n",
    "cv_hyper, cv_precision, cv_recall, cv_fscore = tune(df, 5, True, 'None', 0, False, features, 'mlp', [100,200],\n",
    "                                                    [hiddenlayer-1, hiddenlayer, hiddenlayer+1], [0.03, 0.01])\n",
    "print(\"gs precision:\", cv_precision)\n",
    "print(\"gs recall:\", cv_recall)\n",
    "print(\"gs f-score:\", cv_fscore)\n",
    "print(\"gs hyper:\", cv_hyper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the supposedly best params for interpolation -> worse interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29, 14, 2]\n",
      "\n",
      " precision, recall, f_score: 0.8765432098765432, 0.7802197802197802, 0.8255813953488372\n",
      "[[4.5295e+04 1.0000e+01]\n",
      " [2.0000e+01 7.1000e+01]]\n"
     ]
    }
   ],
   "source": [
    "features = ['Amount', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', \n",
    "            'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28']\n",
    "#maxIter, number of neurons in hidden layer, stepsize\n",
    "params = [200, 14, 0.03]\n",
    "m, p, r, f = TrainTest(trainData,testData,'mlp',features,params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the second best params but search over stepSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 15, 0.06]\n",
      "[29, 15, 2]\n",
      "train: 145268\n",
      "fraud ratio: 0.001700305641985847\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145270\n",
      "fraud ratio: 0.0017002822330832244\n",
      "[0.8850222104454775, 0.7434690639873083, 0.808093533859503]\n",
      "[100, 15, 0.03]\n",
      "[29, 15, 2]\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145268\n",
      "fraud ratio: 0.001700305641985847\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145270\n",
      "fraud ratio: 0.0017002822330832244\n",
      "[0.8799228135698722, 0.7499735589635114, 0.8097678542353672]\n",
      "[100, 15, 0.01]\n",
      "[29, 15, 2]\n",
      "train: 145268\n",
      "fraud ratio: 0.001700305641985847\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145270\n",
      "fraud ratio: 0.0017002822330832244\n",
      "[0.8875697247648467, 0.7466419883659439, 0.8110293406749083]\n",
      "[100, 15, 0.005]\n",
      "[29, 15, 2]\n",
      "train: 145268\n",
      "fraud ratio: 0.001700305641985847\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145270\n",
      "fraud ratio: 0.0017002822330832244\n",
      "[0.8844337530904696, 0.7662612374405076, 0.8211175365101923]\n",
      "gs precision: 0.8844337530904696\n",
      "gs recall: 0.7662612374405076\n",
      "gs f-score: 0.8211175365101923\n",
      "gs hyper: [100, 15, 0.005]\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.csv(\"base_train.csv\", header = True)\n",
    "features = ['Amount', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', \n",
    "            'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', \n",
    "            'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', \n",
    "            'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28']\n",
    "hiddenlayer = int((len(features) + 2) / 2)\n",
    "cv_hyper, cv_precision, cv_recall, cv_fscore = tune(df, 5, True, 'None', 0, False, features, 'mlp', [100],\n",
    "                                                    [15], [0.06, 0.03, 0.01, 0.005])\n",
    "print(\"gs precision:\", cv_precision)\n",
    "print(\"gs recall:\", cv_recall)\n",
    "print(\"gs f-score:\", cv_fscore)\n",
    "print(\"gs hyper:\", cv_hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29, 15, 2]\n",
      "\n",
      " precision, recall, f_score: 0.8735632183908046, 0.8351648351648352, 0.853932584269663\n",
      "[[4.5294e+04 1.1000e+01]\n",
      " [1.5000e+01 7.6000e+01]]\n"
     ]
    }
   ],
   "source": [
    "features = ['Amount', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', \n",
    "            'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28']\n",
    "#maxIter, number of neurons in hidden layer, stepsize\n",
    "params = [100, 15, 0.005]\n",
    "m, p, r, f = TrainTest(trainData,testData,'mlp',features,params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: [100,15,0.03]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection\n",
    "\n",
    "#### p-val(V13, V22, V23, V25, V26) = 0.442, 0.054, 0.147, 0.735, 0.141"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bench mark cv: 0.898414964064903, 0.7662612374405076, 0.8270924537479238\n",
    "#### bench mark interpolation: 0.8735632183908046, 0.8351648351648352, 0.853932584269663"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 remove feature \"V25\" based on T test and run cv with defaults params -> slightly better precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 15, 0.03]\n",
      "[28, 15, 2]\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145268\n",
      "fraud ratio: 0.001700305641985847\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145270\n",
      "fraud ratio: 0.0017002822330832244\n",
      "[0.9121402810595697, 0.7663141195134848, 0.8328924230700581]\n",
      "avg precision: 0.9121402810595697\n",
      "avg recall: 0.7663141195134848\n",
      "avg f-score: 0.8328924230700581\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.csv(\"base_train.csv\", header = True)\n",
    "features = ['Amount', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', \n",
    "            'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V26', 'V27', 'V28']\n",
    "hiddenlayer = int((len(features) + 2) / 2)\n",
    "cv_hyper, cv_precision, cv_recall, cv_fscore = tune(df, 5, True, 'None', 0, False, features, 'mlp', [100], [hiddenlayer], [0.03])\n",
    "print(\"avg precision:\", cv_precision)\n",
    "print(\"avg recall:\", cv_recall)\n",
    "print(\"avg f-score:\", cv_fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Interpolation -> kept the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28, 15, 2]\n",
      "\n",
      " precision, recall, f_score: 0.8735632183908046, 0.8351648351648352, 0.853932584269663\n",
      "[[4.5294e+04 1.1000e+01]\n",
      " [1.5000e+01 7.6000e+01]]\n"
     ]
    }
   ],
   "source": [
    "features = ['Amount', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', \n",
    "            'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V26', 'V27', 'V28']\n",
    "#maxIter, number of neurons in hidden layer, stepsize\n",
    "params = [100, 15, 0.03]\n",
    "m, p, r, f = TrainTest(trainData,testData,'mlp',features,params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 remove feature \"V13\", V25\" based on T test and run cv with default params -> precision and recall trade-off comparing to 4.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 14, 0.03]\n",
      "[27, 14, 2]\n",
      "train: 145268\n",
      "fraud ratio: 0.001700305641985847\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145270\n",
      "fraud ratio: 0.0017002822330832244\n",
      "[0.9037135195958725, 0.7727657324167108, 0.8331255385678298]\n",
      "avg precision: 0.9037135195958725\n",
      "avg recall: 0.7727657324167108\n",
      "avg f-score: 0.8331255385678298\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.csv(\"base_train.csv\", header = True)\n",
    "features = ['Amount', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V14', \n",
    "            'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V26', 'V27', 'V28']\n",
    "hiddenlayer = int((len(features) + 2) / 2)\n",
    "cv_hyper, cv_precision, cv_recall, cv_fscore = tune(df, 5, True, 'None', 0, False, features, 'mlp', [100], [hiddenlayer], [0.03])\n",
    "print(\"avg precision:\", cv_precision)\n",
    "print(\"avg recall:\", cv_recall)\n",
    "print(\"avg f-score:\", cv_fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Interpolation -> 0.006 F-score improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 14, 2]\n",
      "\n",
      " precision, recall, f_score: 0.875, 0.8461538461538461, 0.8603351955307262\n",
      "[[4.5294e+04 1.1000e+01]\n",
      " [1.4000e+01 7.7000e+01]]\n"
     ]
    }
   ],
   "source": [
    "features = ['Amount', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V14', \n",
    "            'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V26', 'V27', 'V28']\n",
    "#maxIter, number of neurons in hidden layer, stepsize\n",
    "params = [100, 14, 0.03]\n",
    "m, p, r, f = TrainTest(trainData,testData,'mlp',features,params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 remove feature \"V13\", \"V23\", \"V25\" based on T test and run cv with default params -> precision and recall trade-off comparing to 4.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 14, 0.03]\n",
      "[26, 14, 2]\n",
      "train: 145268\n",
      "fraud ratio: 0.001700305641985847\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145266\n",
      "fraud ratio: 0.0016934451282474908\n",
      "train: 145270\n",
      "fraud ratio: 0.0017002822330832244\n",
      "[0.9056869927458162, 0.7824960338445267, 0.8395967363320154]\n",
      "avg precision: 0.9056869927458162\n",
      "avg recall: 0.7824960338445267\n",
      "avg f-score: 0.8395967363320154\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.csv(\"base_train.csv\", header = True)\n",
    "features = ['Amount', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V14', \n",
    "            'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V24', 'V26', 'V27', 'V28']\n",
    "hiddenlayer = int((len(features) + 2) / 2)\n",
    "cv_hyper, cv_precision, cv_recall, cv_fscore = tune(df, 5, True, 'None', 0, False, features, 'mlp', [100], [hiddenlayer], [0.03])\n",
    "print(\"avg precision:\", cv_precision)\n",
    "print(\"avg recall:\", cv_recall)\n",
    "print(\"avg f-score:\", cv_fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Interpolation -> 0.001 F-score improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26, 14, 2]\n",
      "\n",
      " precision, recall, f_score: 0.8651685393258427, 0.8461538461538461, 0.8555555555555556\n",
      "[[4.5293e+04 1.2000e+01]\n",
      " [1.4000e+01 7.7000e+01]]\n"
     ]
    }
   ],
   "source": [
    "features = ['Amount', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V14', \n",
    "            'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V24', 'V26', 'V27', 'V28']\n",
    "#maxIter, number of neurons in hidden layer, stepsize\n",
    "params = [100, 14, 0.03]\n",
    "m, p, r, f = TrainTest(trainData,testData,'mlp',features,params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: Use all features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Random Sampling\n",
    "\n",
    "#### bench mark cv: 0.898414964064903, 0.7662612374405076, 0.8270924537479238\n",
    "#### bench mark interpolation: 0.8735632183908046, 0.8351648351648352, 0.853932584269663"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### under sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 15, 0.03]\n",
      "[29, 15, 2]\n",
      "train: 106333\n",
      "fraud ratio: 0.0030000094044181956\n",
      "train: 106666\n",
      "fraud ratio: 0.003000018750117188\n",
      "train: 106333\n",
      "fraud ratio: 0.0030000094044181956\n",
      "train: 106333\n",
      "fraud ratio: 0.0030000094044181956\n",
      "train: 106333\n",
      "fraud ratio: 0.0030000094044181956\n",
      "[0.8644870461818176, 0.7970569620253164, 0.8294037537813084]\n",
      "[100, 15, 0.03]\n",
      "[29, 15, 2]\n",
      "train: 63799\n",
      "fraud ratio: 0.0050000783711343436\n",
      "train: 63799\n",
      "fraud ratio: 0.0050000783711343436\n",
      "train: 63999\n",
      "fraud ratio: 0.005000078126220722\n",
      "train: 63799\n",
      "fraud ratio: 0.0050000783711343436\n",
      "train: 63799\n",
      "fraud ratio: 0.0050000783711343436\n",
      "[0.8391794247215933, 0.8170569620253165, 0.8279704477498036]\n"
     ]
    }
   ],
   "source": [
    "ratio_list = [0.003, 0.005]\n",
    "df = sqlContext.read.csv(\"modeling.csv\", header = True)\n",
    "features = ['Amount', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', \n",
    "            'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', \n",
    "            'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', \n",
    "            'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28']\n",
    "for ratio in ratio_list:\n",
    "    tune(df, 5, True, 'under', ratio, False, features, 'mlp', [100], [15], [0.03])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26, 14, 2]\n",
      "\n",
      " precision, recall, f_score: 0.8588235294117647, 0.8021978021978022, 0.8295454545454546\n",
      "[[4.5293e+04 1.2000e+01]\n",
      " [1.8000e+01 7.3000e+01]]\n"
     ]
    }
   ],
   "source": [
    "trainData = sample(trainData, \"under\", 0.003)\n",
    "\n",
    "features = ['Amount', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V14', \n",
    "            'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V24', 'V26', 'V27', 'V28']\n",
    "#maxIter, number of neurons in hidden layer, stepsize\n",
    "params = [100, 14, 0.03]\n",
    "m, p, r, f = TrainTest(trainData,testData,'mlp',features,params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### over sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 15, 0.03]\n",
      "[29, 15, 2]\n",
      "train: 181809\n",
      "fraud ratio: 0.0029976513813947604\n",
      "train: 181810\n",
      "fraud ratio: 0.0029976348935702108\n",
      "train: 181809\n",
      "fraud ratio: 0.0029976513813947604\n",
      "train: 181809\n",
      "fraud ratio: 0.0029976513813947604\n",
      "train: 181812\n",
      "fraud ratio: 0.002997601918465228\n",
      "[0.8672244847587314, 0.8020569620253164, 0.8333686772588232]\n",
      "[100, 15, 0.03]\n",
      "[29, 15, 2]\n",
      "train: 182174\n",
      "fraud ratio: 0.004995224345954966\n",
      "train: 182175\n",
      "fraud ratio: 0.004995196926032661\n",
      "train: 182174\n",
      "fraud ratio: 0.004995224345954966\n",
      "train: 182174\n",
      "fraud ratio: 0.004995224345954966\n",
      "train: 182177\n",
      "fraud ratio: 0.004995142087091125\n",
      "[0.8195489129858184, 0.8095569620253166, 0.8145222952110646]\n"
     ]
    }
   ],
   "source": [
    "ratio_list = [0.003, 0.005]\n",
    "df = sqlContext.read.csv(\"modeling.csv\", header = True)\n",
    "features = ['Amount', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', \n",
    "            'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', \n",
    "            'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', \n",
    "            'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28']\n",
    "for ratio in ratio_list:\n",
    "    tune(df, 5, True, 'over', ratio, False, features, 'mlp', [100], [15], [0.03])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26, 14, 2]\n",
      "\n",
      " precision, recall, f_score: 0.872093023255814, 0.8241758241758241, 0.847457627118644\n",
      "[[4.5294e+04 1.1000e+01]\n",
      " [1.6000e+01 7.5000e+01]]\n"
     ]
    }
   ],
   "source": [
    "trainData = sample(trainData, \"over\", 0.003)\n",
    "\n",
    "features = ['Amount', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V14', \n",
    "            'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V24', 'V26', 'V27', 'V28']\n",
    "#maxIter, number of neurons in hidden layer, stepsize\n",
    "params = [100, 14, 0.03]\n",
    "m, p, r, f = TrainTest(trainData,testData,'mlp',features,params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: Use original ratio; no sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. OOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29, 15, 2]\n",
      "\n",
      " precision, recall, f_score: 0.9310344827586207, 0.7297297297297297, 0.8181818181818181\n",
      "[[5.6668e+04 4.0000e+00]\n",
      " [2.0000e+01 5.4000e+01]]\n"
     ]
    }
   ],
   "source": [
    "features = ['Amount', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', \n",
    "            'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', \n",
    "            'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', \n",
    "            'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28']\n",
    "#maxIter, number of neurons in hidden layer, stepsize\n",
    "params = [100, 15, 0.03]\n",
    "m, p, r, f = TrainTest(trainData,oot,'mlp',features,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
